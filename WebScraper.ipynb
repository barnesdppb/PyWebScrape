{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen (\"http://www.pythonscraping.com/exercises/exercise1.html\")\n",
    "bsObj = BeautifulSoup(html.read(), \"lxml\");\n",
    "print(bsObj.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found\n"
     ]
    }
   ],
   "source": [
    "# Include some code to handle errors - in case the website doesn't exist \n",
    "# or the HTML tag doesn't exist\n",
    "try:\n",
    "    badContent = bsObj.nonExisting.anotherTag\n",
    "except AttributeError as e:\n",
    "    print(\"Tag not found\")\n",
    "else:\n",
    "    if badContent == None:\n",
    "        print (\"Tag was not found\")\n",
    "    else:\n",
    "        print(badContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "# Adding it all together\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print (\"Page not found\")\n",
    "        return none\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "        title = bsObj.h1\n",
    "    except AttributeError as e:\n",
    "        print (\"No such tag\")\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/exercises/exercise1.html\")\n",
    "if title == None:\n",
    "    print (\"Title couldn't be found\")\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to start at Kevin Bacon's wiki page and crawl to 10 random wiki pages\n",
    "# based on links from that page\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "# regex - ?! is negative lookahead, so does NOT contain a colon\n",
    "random.seed(datetime.datetime.now())\n",
    "def getLinks (articleUrl):\n",
    "    html = urlopen(\"http://en.wikipedia.org\" + articleUrl)\n",
    "    bsObj = bs(html, \"lxml\")\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", \n",
    "        href = re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "\n",
    "links = getLinks(\"/wiki/Kevin_Bacon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Republican_National_Convention\n",
      "/wiki/California_Republican_Party\n",
      "/wiki/Charles_Evans_Hughes\n",
      "/wiki/2016_Republican_National_Convention\n",
      "/wiki/Multi-level_marketing\n",
      "/wiki/Statute\n",
      "/wiki/Attorney_at_law\n",
      "/wiki/Republic_of_Ireland\n",
      "/wiki/List_of_sovereign_states_and_dependent_territories_by_population_density\n",
      "/wiki/Belarus\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    newArticle = links[random.randint(0, len(links) - 1)].attrs[\"href\"]\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rugby/report?gameId=290778&league=242041\n"
     ]
    }
   ],
   "source": [
    "# TEST - go to the scrum.com fixtures page and extract the results links. Info is in JSON format, so pull that \n",
    "# from the HTML then parse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "\n",
    "# This works for pages that have a single match, probably not for multiple matches\n",
    "date = '20170302'\n",
    "def getLinks (articleUrl):\n",
    "    html = urlopen(\"http://www.espn.co.uk/rugby/fixtures/_/date/\" + articleUrl)\n",
    "    bsObj = bs(html, \"lxml\")\n",
    "    return bsObj.find(\"section\", {\"id\":\"pane-main\"}).findAll(\"script\", {\"type\":\"text/javascript\"})\n",
    "    return bsObj.find(\"section\", {\"id\":\"pane-main\"})\n",
    "\n",
    "\n",
    "tst = getLinks(date)\n",
    "for t in tst:\n",
    "    json_text = re.search(r'^\\s*window\\.__INITIAL_STATE__\\s*=\\s*({.*?})\\s*;\\s*$',\n",
    "                      t.string, flags=re.DOTALL | re.MULTILINE).group(1)\n",
    "\n",
    "json_out = json.loads(json_text)\n",
    "match = json_out['schedule']['groups'][0]['complete'][0]['result']['href']\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rugby/report?gameId=290778&league=242041'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_out['schedule']['groups'][0]['complete'][0]['result']['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing a day with multiple matches\n",
    "# TEST - go to the scrum.com fixtures page and extract the results links. Info is in JSON format, so pull that \n",
    "# from the HTML then parse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "\n",
    "# This works for pages that have a single match, probably not for multiple matches\n",
    "date = '20170304'\n",
    "def getLinks (articleUrl):\n",
    "    html = urlopen(\"http://www.espn.co.uk/rugby/fixtures/_/date/\" + articleUrl)\n",
    "    bsObj = bs(html, \"lxml\")\n",
    "    return bsObj.find(\"section\", {\"id\":\"pane-main\"}).findAll(\"script\", {\"type\":\"text/javascript\"})\n",
    "\n",
    "tst = getLinks(date)\n",
    "for t in tst:\n",
    "    json_text = re.search(r'^\\s*window\\.__INITIAL_STATE__\\s*=\\s*({.*?})\\s*;\\s*$',\n",
    "                      t.string, flags=re.DOTALL | re.MULTILINE).group(1)\n",
    "\n",
    "json_out = json.loads(json_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rugby/report?gameId=290088&league=267979\n",
      "/rugby/report?gameId=290089&league=267979\n",
      "/rugby/match?gameId=290364&league=270559\n",
      "/rugby/match?gameId=290363&league=270559\n",
      "/rugby/match?gameId=290366&league=270559\n",
      "/rugby/match?gameId=290365&league=270559\n",
      "/rugby/report?gameId=290524&league=270557\n",
      "/rugby/report?gameId=290520&league=270557\n",
      "/rugby/report?gameId=290523&league=270557\n",
      "/rugby/report?gameId=290786&league=242041\n",
      "/rugby/report?gameId=290785&league=242041\n",
      "/rugby/report?gameId=290784&league=242041\n",
      "/rugby/match?gameId=290783&league=242041\n",
      "/rugby/report?gameId=290782&league=242041\n",
      "/rugby/report?gameId=290781&league=242041\n"
     ]
    }
   ],
   "source": [
    "# Structure like so: json_out['schedule']['groups'][m]['complete'][n]['result']['href']\n",
    "# m is the league in question and n is the game within that league, iterate through both for all results\n",
    "\n",
    "json_match = json_out['schedule']['groups']\n",
    "matches = []\n",
    "\n",
    "for m in json_match:\n",
    "    for c in m['complete']:\n",
    "        matches.append(c['result']['href'])\n",
    "        \n",
    "for m in matches:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just pick a single match link to look at\n",
    "match_link = matches[0]\n",
    "\n",
    "# New function, very similar to the above, to extract the stats links\n",
    "def getStatsLinks (articleUrl):\n",
    "    html = urlopen(\"http://www.espn.co.uk\" + articleUrl)\n",
    "    bsObj = bs(html, \"lxml\")\n",
    "    return bsObj.find(\"section\", {\"id\":\"pane-main\"}).findAll(\"script\", {\"type\":\"text/javascript\"})\n",
    "\n",
    "# Saved to list this time but still only one element\n",
    "tst = getStatsLinks(match_link)\n",
    "json_out = []\n",
    "for t in tst:\n",
    "    json_text = re.search(r'^\\s*window\\.__INITIAL_STATE__\\s*=\\s*({.*?})\\s*;\\s*$',\n",
    "                      t.string, flags=re.DOTALL | re.MULTILINE).group(1)\n",
    "    json_out.append(json.loads(json_text))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are tall the match related links, we are interested in the match\n",
    "# and player stats, as well as lineups and possibly table\n",
    "json_out[0]['gamePackage']['links']\n",
    "\n",
    "for j in json_out[0]['gamePackage']['links']:\n",
    "    if j['pageType'] == 'matchstats':\n",
    "        match_s = j['href']\n",
    "    elif j['pageType'] == 'playerstats':\n",
    "        player_s = j['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getStats(statsUrl):\n",
    "    html = urlopen(\"http://www.espn.co.uk\" + statsUrl)\n",
    "    bsObj = bs(html, \"lxml\")  \n",
    "    return bsObj.find(\"section\", {\"id\":\"pane-main\"}).findAll(\"script\", {\"type\":\"text/javascript\"})\n",
    "\n",
    "p_stats = getStats(player_s)\n",
    "\n",
    "# json.loads turns the JSON object into a Python dict - so it's now a dict and follows those rules \n",
    "stat_out = []\n",
    "# This is length 1 but has to be done as if we don't iterate though it's of type \"ResultsSet\"\n",
    "# which we can do do any operations on. So after this it will take the __INITIAL_STATE__ variable,\n",
    "# grab the JSON object and make it into a Python dict\n",
    "for p in p_stats:\n",
    "    json_text = re.search(r'^\\s*window\\.__INITIAL_STATE__\\s*=\\s*({.*?})\\s*;\\s*$',\n",
    "                      t.string, flags=re.DOTALL | re.MULTILINE).group(1)\n",
    "    stat_out.append(json.loads(json_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page appProps\n",
      "page edition\n",
      "page template\n",
      "page params\n",
      "routing location\n",
      "gamePackage matchStats\n",
      "gamePackage matchAwayForm\n",
      "gamePackage leagueUid\n",
      "gamePackage matchLineUp\n",
      "gamePackage matchDiscipline\n",
      "gamePackage standings\n",
      "gamePackage HeadToHeadNode\n",
      "gamePackage meta\n",
      "gamePackage gameState\n",
      "gamePackage matchDetails\n",
      "gamePackage matchGlossary\n",
      "gamePackage article\n",
      "gamePackage polling\n",
      "gamePackage commentaryFeedback\n",
      "gamePackage matchConversation\n",
      "gamePackage matchHomeForm\n",
      "gamePackage matchAttacking\n",
      "gamePackage matchEvents\n",
      "gamePackage gameStateClass\n",
      "gamePackage matchSummary\n",
      "gamePackage links\n",
      "gamePackage loading\n",
      "gamePackage headToHead\n",
      "gamePackage analytics\n",
      "gamePackage matchCommentary\n",
      "gamePackage gameStrip\n",
      "gamePackage matchDefending\n",
      "gamePackage news\n",
      "gamePackage showGameDetailFooter\n"
     ]
    }
   ],
   "source": [
    "for s in stat_out[0]:\n",
    "    for i in stat_out[0][s]:\n",
    "        print(s, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for g in stat_out[0][\"news\"]:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'captain': False,\n",
      " 'cleanbreaks': {'name': 'Clean Breaks', 'value': '0'},\n",
      " 'conversiongoals': {'name': 'Conversion Goals', 'value': '0'},\n",
      " 'defendersbeaten': {'name': 'Defenders Beaten', 'value': '0'},\n",
      " 'dropgoalsconverted': {'name': 'Drop Goals Converted', 'value': '0'},\n",
      " 'eventTimes': {'3': [\"41'+3\"]},\n",
      " 'homeAway': 'home',\n",
      " 'id': '82104',\n",
      " 'kicks': {'name': 'Kicks', 'value': '2'},\n",
      " 'lineoutswon': {'name': 'Lineouts Won', 'value': '0'},\n",
      " 'lineoutwonsteal': {'name': 'Lineout Won Steal', 'value': '0'},\n",
      " 'metres': {'name': 'Metres Run', 'value': '15'},\n",
      " 'missedtackles': {'name': 'Missed Tackles', 'value': '1'},\n",
      " 'name': 'Tom Homer',\n",
      " 'number': '15',\n",
      " 'offload': {'name': 'Offload', 'value': '0'},\n",
      " 'onPitch': False,\n",
      " 'passes': {'name': 'Passes', 'value': '6'},\n",
      " 'penalties': {'name': 'penalties', 'value': 1},\n",
      " 'penaltiesconceded': {'name': 'Penalties Conceded', 'value': '0'},\n",
      " 'penaltygoals': {'name': 'Penalty Goals', 'value': '1'},\n",
      " 'points': {'name': 'Points', 'value': '3'},\n",
      " 'position': 'FB',\n",
      " 'redcards': {'name': 'Red Cards', 'value': '0'},\n",
      " 'runs': {'name': 'Runs', 'value': '6'},\n",
      " 'subToolTip': '',\n",
      " 'subbed': False,\n",
      " 'tackles': {'name': 'Tackles', 'value': '2'},\n",
      " 'tries': {'name': 'Tries', 'value': '0'},\n",
      " 'tryassists': {'name': 'Try Assists', 'value': '0'},\n",
      " 'turnoversconceded': {'name': 'Turnovers Conceded', 'value': '0'},\n",
      " 'url': 'http://en.espn.co.uk/sport/rugby/player/82104.html',\n",
      " 'wasActive': True,\n",
      " 'yellowcards': {'name': 'Yellow Cards', 'value': '0'}}\n"
     ]
    }
   ],
   "source": [
    "vals = ['name', 'cleanbreaks']\n",
    "data = stat_out[0][\"gamePackage\"][\"matchLineUp\"][\"home\"][\"team\"]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reserves\n",
      "logo\n",
      "name\n",
      "team\n"
     ]
    }
   ],
   "source": [
    "for s in stat_out[0][\"gamePackage\"][\"matchLineUp\"][\"home\"]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dct contains player data for a single game\n",
    "dct = data[0]\n",
    "\n",
    "# remove entries we don't want - url is useless and eventTimes doesn't conform to the required structure\n",
    "# these are the keys, i.e. table column names\n",
    "cols = list(dct.keys())\n",
    "cols = list(filter(lambda c: c != 'eventTimes', cols))\n",
    "cols = list(filter(lambda c: c != 'url', cols))\n",
    "\n",
    "# these are used for the SQL query, so everything gets inserted at once\n",
    "placeholders = ', '.join(['%s'] * len(cols))\n",
    "columns = ', '.join(cols)\n",
    "\n",
    "# These are the values to be inserted into the table\n",
    "vals = []\n",
    "for d in dct:\n",
    "    if d in cols:\n",
    "        if type(dct[d]) is dict:\n",
    "            vals.append(dct[d]['value'])\n",
    "        else:\n",
    "            vals.append(dct[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query to create the table in MySQL - this will need a match and player identifier\n",
    "sql = \"\"\"drop table if exists scraping.r_stats;\n",
    "        create table r_stats(id bigint(7) not null\n",
    "                            , name varchar(200)\n",
    "                            , number varchar(10)\n",
    "                            , position varchar(10)\n",
    "                            , captain varchar(10)\n",
    "                            , subbed varchar(10)\n",
    "                            , homeAway varchar(10)\n",
    "                            , subToolTip varchar(10)\n",
    "                            , onPitch varchar(10)\n",
    "                            , wasActive varchar(10)\n",
    "                            , tries bigint(7)\n",
    "                            , tryassists bigint(7)\n",
    "                            , points bigint(7)\n",
    "                            , kicks bigint(7)\n",
    "                            , passes bigint(7)\n",
    "                            , runs bigint(7)\n",
    "                            , metres bigint(7)\n",
    "                            , cleanbreaks bigint(7)\n",
    "                            , defendersbeaten bigint(7)\n",
    "                            , offload bigint(7)\n",
    "                            , lineoutwonsteal bigint(7)\n",
    "                            , turnoversconceded bigint(7)\n",
    "                            , tackles bigint(7)\n",
    "                            , missedtackles bigint(7)\n",
    "                            , lineoutswon bigint(7)\n",
    "                            , penaltiesconceded bigint(7)\n",
    "                            , yellowcards bigint(7)\n",
    "                            , redcards bigint(7)\n",
    "                            , penalties bigint(7)\n",
    "                            , penaltygoals bigint(7)\n",
    "                            , conversiongoals bigint(7)\n",
    "                            , dropgoalsconverted bigint(7)\n",
    "                            , primary key(id));\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the data into the table\n",
    "\n",
    "# Set up the connection\n",
    "import pymysql\n",
    "conn = pymysql.connect(host = '127.0.0.1', port = 3306,\n",
    "                       user = 'root', passwd = '', db = 'mysql')\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scraping\")\n",
    "cur.execute(sql)\n",
    "\n",
    "#sql_update = \"insert into r_stats (%s) values (%s)\" % (columns, placeholders)\n",
    "#cur.execute(sql_update, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attempt to do same for every player the match:\n",
    "# dct contains player data for a single game\n",
    "\n",
    "teams, players = ['home', 'away'], ['team', 'reserves']\n",
    "\n",
    "for t in teams:\n",
    "    for p in players:\n",
    "        data = stat_out[0][\"gamePackage\"][\"matchLineUp\"][t][p]\n",
    "\n",
    "        for d in data:\n",
    "            dct = d\n",
    "\n",
    "            # remove entries we don't want - url is useless and eventTimes doesn't conform to the required structure\n",
    "            # these are the keys, i.e. table column names\n",
    "            cols = list(dct.keys())\n",
    "            cols = list(filter(lambda c: c != 'eventTimes', cols))\n",
    "            cols = list(filter(lambda c: c != 'url', cols))\n",
    "\n",
    "            # these are used for the SQL query, so everything gets inserted at once\n",
    "            placeholders = ', '.join(['%s'] * len(cols))\n",
    "            columns = ', '.join(cols)\n",
    "\n",
    "            # These are the values to be inserted into the table\n",
    "            vals = []\n",
    "            for d in dct:\n",
    "                if d in cols:\n",
    "                    if type(dct[d]) is dict:\n",
    "                        vals.append(dct[d]['value'])\n",
    "                    else:\n",
    "                        vals.append(dct[d])\n",
    "\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"USE scraping\")\n",
    "            #cur.execute(sql)\n",
    "\n",
    "            sql_update = \"insert into r_stats (%s) values (%s)\" % (columns, placeholders)\n",
    "            cur.execute(sql_update, vals)\n",
    "            conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
